ComputeNewHypothesis(new_h)
names(mushroom_data)
source('ComputeNewHypothesis.R')
ComputeNewHypothesis(new_h)
new_h <- ComputeNewHypothesis(new_h)
ComputeNewHypothesis(new_h)
Accuracy(new_h)
Accuracy(last_h)
source('Satisfies.R')
source('ComputeNewHypothesis.R')
source('Accuracy.R')
mushroom_data <- read.csv('../datasets/mushroom_data.txt')
# Sample data
train = sample(1:nrow(mushroom_data), 1000)
mushroom_data <- mushroom_data[train, ]
n_attr <- ncol(mushroom_data) - 1
# List unique values for each attribute
unique_val <- NULL
for(i in 1:n_attr+1) {
unique_val <- append(unique_val, list(as.character(unique(mushroom_data[, i]))))
}
# Set containing all the hypotheses
H <- NULL
# Insert the most general hypothesis as the first
last_h <- rep('?', n_attr)
H <- rbind(H, last_h)
t <- 0
new_h <- ComputeNewHypothesis(last_h)
while(Accuracy(new_h) > Accuracy(last_h)) {
last_h <- new_h
new_h <- ComputeNewHypothesis(last_h)
H <- rbind(H, last_h)
}
H
new_h
Accuracy(new_h)
Accuracy(last_h)
source('Satisfies.R')
source('ComputeNewHypothesis.R')
source('Accuracy.R')
mushroom_data <- read.csv('../datasets/mushroom_data.txt')
# Sample data
train = sample(1:nrow(mushroom_data), 1000)
mushroom_data <- mushroom_data[train, ]
n_attr <- ncol(mushroom_data) - 1
# List unique values for each attribute
unique_val <- NULL
for(i in 1:n_attr+1) {
unique_val <- append(unique_val, list(as.character(unique(mushroom_data[, i]))))
}
# Set containing all the hypotheses
H <- NULL
# Insert the most general hypothesis as the first
last_h <- rep('?', n_attr)
H <- rbind(H, last_h)
t <- 0
new_h <- ComputeNewHypothesis(last_h)
while(Accuracy(new_h) >= Accuracy(last_h)) {
last_h <- new_h
new_h <- ComputeNewHypothesis(last_h)
H <- rbind(H, last_h)
}
H
unique(mushroom_data[, 1])
unique(mushroom_data[, 2])
Accuracy(H[1, ])
Accuracy(H[2, ])
Accuracy(H[3, ])
Accuracy(H[4, ])
Accuracy(H[10, ])
Accuracy(H[14, ])
source('ComputeNewHypothesis.R')
ComputeNewHypothesis(H[1, ])
source('ComputeNewHypothesis.R')
ComputeNewHypothesis(H[1, ])
Satisfies(H[1, ], mushroom_data[1, ])
Satisfies(H[1, ], mushroom_data[2, ])
Satisfies(H[1, ], mushroom_data[20, ])
Satisfies(H[1, ], mushroom_data[200, ])
Accuracy(H[1, ])
H[1, ]
t <- H[1, ]
t
t[10]
unique(mushroom_data[10, ])
unique(mushroom_data[, 10])
t[9] <- "b"
t
Accuracy(t)
t[9] <- "x"
t
Accuracy(t)
source('Satisfies.R')
Accuracy(t)
source('Satisfies.R')
Accuracy(t)
source('Satisfies.R')
Accuracy(t)
source('Satisfies.R')
Accuracy(t)
source('Satisfies.R')
Accuracy(t)
source('Satisfies.R')
Accuracy(t)
source('Satisfies.R')
Accuracy(t)
source('Satisfies.R')
source('ComputeNewHypothesis.R')
source('Accuracy.R')
mushroom_data <- read.csv('../datasets/mushroom_data.txt')
# Sample data
train = sample(1:nrow(mushroom_data), 1000)
mushroom_data <- mushroom_data[train, ]
n_attr <- ncol(mushroom_data) - 1
# List unique values for each attribute
unique_val <- NULL
for(i in 1:n_attr+1) {
unique_val <- append(unique_val, list(as.character(unique(mushroom_data[, i]))))
}
# Set containing all the hypotheses
H <- NULL
# Insert the most general hypothesis as the first
last_h <- rep('?', n_attr)
H <- rbind(H, last_h)
t <- 0
new_h <- ComputeNewHypothesis(last_h)
source('Satisfies.R')
source('ComputeNewHypothesis.R')
source('Accuracy.R')
mushroom_data <- read.csv('../datasets/mushroom_data.txt')
# Sample data
train = sample(1:nrow(mushroom_data), 1000)
mushroom_data <- mushroom_data[train, ]
n_attr <- ncol(mushroom_data) - 1
# List unique values for each attribute
unique_val <- NULL
for(i in 1:n_attr+1) {
unique_val <- append(unique_val, list(as.character(unique(mushroom_data[, i]))))
}
# Set containing all the hypotheses
H <- NULL
# Insert the most general hypothesis as the first
last_h <- rep('?', n_attr)
H <- rbind(H, last_h)
t <- 0
new_h <- ComputeNewHypothesis(last_h)
while(Accuracy(new_h) >= Accuracy(last_h)) {
last_h <- new_h
new_h <- ComputeNewHypothesis(last_h)
H <- rbind(H, last_h)
}
dim(H)
H
Accuracy(H[1, ])
Accuracy(H[2, ])
Accuracy(H[3, ])
Accuracy(H[4, ])
Accuracy(H[10, ])
Accuracy(H[20, ])
t <- H[11]
t
t <- H[11, ]
t
t[11] <- 'z'
t
Accuracy(H[11, ])
Accuracy(t)
unique(mushroom_data[, 12])
source('Satisfies.R')
Accuracy(t)
Accuracy(H[11, ])
remove(Satisfes)
remove(Satisifes)
Satisfies(t, mushroom_data[11, ])
Satisfies(t, mushroom_data[12, ])
Satisfies(t, mushroom_data[1, ])
Satisfies(t, mushroom_data[2, ])
Satisfies(t, mushroom_data[3, ])
Satisfies(t, mushroom_data[4, ])
for(i in 1:nrow(mushroom_data)) {}
r = 0
for(i in 1:nrow(mushroom_data)) { if(Satisfies(t, mushroom_data[i, ])) { r = r + 1}}
r
source('Accuracy.R')
Accuracy(t)
source('Accuracy.R')
H
Accuracy(H[1, ])
rm(list=ls(all=TRUE))
setwd('../p9/')
list.files()
# Required for simple_triplet_matrix
library(slam)
source('DocFreq.R')
source('ComputerIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
word_per_doc_count[word_per_doc_count$V1 <= 1, ]
document_labels[document_labels$V1 == 1]
document_labels[document_labels$V1 == 1, ]
document_labels[document_labels$V1 == 2, ]
document_labels[document_labels$V1 == 3, ]
document_labels[document_labels$V1 == 4, ]
document_labels[document_labels$V1 == 5, ]
unique[document_labels]
unique(document_labels)
x=unique(document_labels)
x
dim(x)
x[1,0]
rownames(x)
word_per_doc_count[word_per_doc_count$V1 == 1:2]
word_per_doc_count[word_per_doc_count$V1 == 1:2, ]
1:(4+2)
# Required for simple_triplet_matrix
library(slam)
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels = document_labels[doc_labels]
new_word_per_doc_count = word_per_doc_count[word_per_doc_count$V1 == doc_labels]
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
# Required for simple_triplet_matrix
library(slam)
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels = document_labels[doc_labels]
new_word_per_doc_count = word_per_doc_count[word_per_doc_count$V1 == doc_labels, ]
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
temp
for(i in rownames(temp)) { doc_lables = as.numeric(i):(as.numeric(i) + d)}
doc_lables
remove(doc_lables)
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels = document_labels[doc_labels]
new_word_per_doc_count = word_per_doc_count[word_per_doc_count$V1 == doc_labels, ]
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
# Required for simple_triplet_matrix
library(slam)
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels = document_labels[doc_labels]
new_word_per_doc_count = word_per_doc_count[word_per_doc_count$V1 == doc_labels, ]
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels = document_labels[doc_labels, ]
new_word_per_doc_count = word_per_doc_count[word_per_doc_count$V1 == doc_labels, ]
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
warnings()
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
x=word_per_doc_count[word_per_doc_count$V1 == 1:5]
x=word_per_doc_count[word_per_doc_count$V1 == 1:5, ]
x=word_per_doc_count[word_per_doc_count$V1 == 1000:5000, ]
x=word_per_doc_count[word_per_doc_count$V1 == 1000:5000, ]
dim(x)
dim(word_per_doc_count)
x=word_per_doc_count[word_per_doc_count$V1 == 10:50, ]
x=word_per_doc_count[word_per_doc_count$V1 == 1:50, ]
x=word_per_doc_count[word_per_doc_count$V1 == 1:5, ]
x=word_per_doc_count[word_per_doc_count$V1 == 1:6, ]
dim(x)
unique(x[1,], )
unique(, 1)
unique(x[], 1])
unique(x[, 1])
x
x=word_per_doc_count[word_per_doc_count$V1>= 6 && word_per_doc_count$V1<=100, ]
dim(x)
x=word_per_doc_count[word_per_doc_count$V1 >= 6 && word_per_doc_count$V1 <= 100, ]
dim(x)
x=word_per_doc_count[word_per_doc_count$V1 == 1:102, ]
x
x
x=word_per_doc_count[word_per_doc_count$V1 %in% 1:102, ]
dim(x)
# Required for simple_triplet_matrix
library(slam)
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels = document_labels[doc_labels, ]
new_word_per_doc_count = word_per_doc_count[word_per_doc_count$V1 %in% doc_labels, ]
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
tfidf <- ComputeTFIDF()
document_labels
temp
rownames(temp)
1:10
# Required for simple_triplet_matrix
library(slam)
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels <- rbind(new_document_labels, document_labels[doc_labels, ])
new_word_per_doc_count = rbind(new_word_per_doc_count,
word_per_doc_count[word_per_doc_count$V1 %in% doc_labels, ])
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
tfidf <- ComputeTFIDF()
tfidf
dim(document_labels)
document_labels
document_labels <- read.table('../datasets/newsgroup/train.label')
document_labels[1:2, ]
rbind(document_labels[1:2, ], document_labels[5:6, ])
cbind(document_labels[1:2, ], document_labels[5:6, ])
typeof(document_labels[1:2, ])
typeof(document_labels)
x = list()
x = document_labels[1:2, ]
typeof(x)
x = list(document_labels[1:2, ])
x
x = as.data.frame(document_labels[1:2, ])
x
document_labels
x = as.data.frame(document_labels[1:2, ])
x = rbind(x, document_labels[5:6, ])
x
x = rbind(x, document_labels[5:60, ])
x
x = rbind(x, as.data.frame(document_labels[5:60, ]))
as.table(document_labels[1:2,])
as.list(document_labels[1:2,])
list(document_labels[1:2,])
x=list(document_labels[1:2,])
rbind(x, list(document_labels[5:6,]))
x=rbind(x, list(document_labels[5:6,]))
x
unique(word_per_doc_count[, 1])
document_labels
document_labels[1:2]
document_labels[1:2,]
document_labels[1:4,]
document_labels[1:5,]
c(document_labels[1:2,], document_labels[500:505])
c(document_labels[1:2,], document_labels[500:505,])
# Required for simple_triplet_matrix
library(slam)
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels <- c(new_document_labels, document_labels[doc_labels, ])
new_word_per_doc_count = rbind(new_word_per_doc_count,
word_per_doc_count[word_per_doc_count$V1 %in% doc_labels, ])
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
length(document_labels)
tfidf <- ComputeTFIDF()
document_labels <- as.data.frame(document_labels)
names(document_labels)
document_labels[1:2, ]
dim(document_labels)
# Required for simple_triplet_matrix
library(slam)
source('DocFreq.R')
source('ComputeIDF.R')
source('ComputeTFIDF.R')
source('SphericalKMeans.R')
source('FarthestFirstPoint.R')
word_per_doc_count <- read.table('../datasets/newsgroup/train.data')
vocabulary <- read.table('../datasets/newsgroup/vocabulary.txt')
document_labels <- read.table('../datasets/newsgroup/train.label')
# Take d+1 documents of each category
d <- 9
temp <- unique(document_labels)
new_word_per_doc_count <- NULL
new_document_labels <- NULL
for(i in rownames(temp)) {
doc_labels = as.numeric(i):(as.numeric(i) + d)
new_document_labels <- c(new_document_labels, document_labels[doc_labels, ])
new_word_per_doc_count = rbind(new_word_per_doc_count,
word_per_doc_count[word_per_doc_count$V1 %in% doc_labels, ])
}
document_labels <- new_document_labels
word_per_doc_count <- new_word_per_doc_count
remove(new_document_labels)
remove(new_word_per_doc_count)
document_labels <- as.data.frame(document_labels)
